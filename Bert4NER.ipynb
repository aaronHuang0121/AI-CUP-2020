{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import load_metric\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from transformers import BertConfig, BertTokenizer, BertForTokenClassification, AdamW, get_linear_schedule_with_warmup\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from seqeval.metrics import f1_score, accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify and specify the GPU as the device, later in training loop we will load data into device\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "N_GPU = torch.cuda.device_count()\n",
    "GPU_NAME = torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化建立entity types list\n",
    "# return ENTITY_TYPES, TAGS2ID, ID2TAGS\n",
    "def initial_entity_types(tags):\n",
    "    ALL_TAGS = []\n",
    "    ALL_TAGS.append('O')\n",
    "    for tag in tags:\n",
    "        ALL_TAGS.append('B-' + tag)\n",
    "        ALL_TAGS.append('I-' + tag)\n",
    "        #ALL_TAGS.append('E-' + tag) #\n",
    "        #ALL_TAGS.append('S-' + tag) #\n",
    "    TAGS2ID = {tag: id for id, tag in enumerate(ALL_TAGS)}\n",
    "    ID2TAGS = {id: tag for tag, id in TAGS2ID.items()}\n",
    "    return ALL_TAGS, TAGS2ID, ID2TAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENTITY TYPES\n",
    "tags = ['name', 'location', 'time', 'contact', 'ID', 'profession', 'biomarker', 'family', 'clinical_event', 'special_skills', 'unique_treatment', 'account', 'organization', 'education', 'money', 'belonging_mark', 'med_exam', 'others']\n",
    "ENTITY_TYPES, TAGS2ID, ID2TAGS = initial_entity_types(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': 0,\n",
       " 'B-name': 1,\n",
       " 'I-name': 2,\n",
       " 'B-location': 3,\n",
       " 'I-location': 4,\n",
       " 'B-time': 5,\n",
       " 'I-time': 6,\n",
       " 'B-contact': 7,\n",
       " 'I-contact': 8,\n",
       " 'B-ID': 9,\n",
       " 'I-ID': 10,\n",
       " 'B-profession': 11,\n",
       " 'I-profession': 12,\n",
       " 'B-biomarker': 13,\n",
       " 'I-biomarker': 14,\n",
       " 'B-family': 15,\n",
       " 'I-family': 16,\n",
       " 'B-clinical_event': 17,\n",
       " 'I-clinical_event': 18,\n",
       " 'B-special_skills': 19,\n",
       " 'I-special_skills': 20,\n",
       " 'B-unique_treatment': 21,\n",
       " 'I-unique_treatment': 22,\n",
       " 'B-account': 23,\n",
       " 'I-account': 24,\n",
       " 'B-organization': 25,\n",
       " 'I-organization': 26,\n",
       " 'B-education': 27,\n",
       " 'I-education': 28,\n",
       " 'B-money': 29,\n",
       " 'I-money': 30,\n",
       " 'B-belonging_mark': 31,\n",
       " 'I-belonging_mark': 32,\n",
       " 'B-med_exam': 33,\n",
       " 'I-med_exam': 34,\n",
       " 'B-others': 35,\n",
       " 'I-others': 36}"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TAGS2ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using model\n",
    "MODEL = dict(\n",
    "    MODEL_VERSION = 'bert-base-chinese',\n",
    "    BATCH_SIZE = 32,\n",
    "    EPOCHS = 6,\n",
    "    lr = 2e-5,\n",
    "    eps = 1e-8,\n",
    "    WEIGHT_DECAY = 0.1,\n",
    "    MAX_GRAD_NORM = 1.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRIC = load_metric(\"seqeval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練資料讀取\n",
    "def loadInputFile(path):\n",
    "    trainingset = list()  # store trainingset [content,content,...]\n",
    "    position = list()  # store position [article_id, start_pos, end_pos, entity_text, entity_type, ...]\n",
    "    mentions = dict()  # store mentions[mention] = Type\n",
    "\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        file_text = f.read().encode('utf-8').decode('utf-8-sig')\n",
    "    \n",
    "    datas = file_text.split('\\n\\n--------------------\\n\\n')[:-1]\n",
    "    \n",
    "    for data in datas:\n",
    "        data = data.split('\\n')\n",
    "        content = data[0]\n",
    "        trainingset.append(content)\n",
    "        annotations = data[1:]\n",
    "        for annot in annotations[1:]:\n",
    "            annot = annot.split('\\t') #annot = article_id, start_pos, end_pos, entity_text, entity_type\n",
    "            position.extend(annot)\n",
    "            mentions[annot[3]] = annot[4]\n",
    "    \n",
    "    return trainingset, position, mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將position轉為dataframe格式\n",
    "def tranfer2dataframe(position):\n",
    "    preDataframe = [position[i:i + 5] for i in range(0, len(position), 5)]\n",
    "    df = pd.DataFrame(preDataframe, columns=['article_id', 'start_pos', 'end_pos', 'entity_text', 'entity_type'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將IOB NER加到tuple後\n",
    "def FormatEntity(origin_data, position):\n",
    "    origin_len = len(origin_data[0])\n",
    "    result = origin_data.copy()\n",
    "    for index, row in position.iterrows():\n",
    "        start_pos = int(row['start_pos'])\n",
    "        end_pos = int(row['end_pos'])\n",
    "        for i in range(start_pos, end_pos):\n",
    "            if i == start_pos:\n",
    "                entity_type = 'B-' + row['entity_type']\n",
    "            else:\n",
    "                entity_type = 'I-' + row['entity_type']\n",
    "            result[i] += (entity_type, )\n",
    "    return list(map(lambda x: x+('O', ) if len(x) == origin_len else x, result))    # 將沒有標記的標記上O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    tokens = [char for char in text]\n",
    "    tokens = list(map(lambda char: (char, ), tokens))\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 讀取資料pipline\n",
    "def loadData(path):\n",
    "    trainingset, position, mentions = loadInputFile(path)\n",
    "    df_position = tranfer2dataframe(position)\n",
    "    words_list = list(map(lambda text: tokenizer(text), trainingset))\n",
    "    traindata_list = []\n",
    "    for index in range(len(words_list)):\n",
    "        index_position = df_position[df_position.article_id == str(index)]\n",
    "        traindata_list.append(FormatEntity(words_list[index], index_position))  # 要放在最後面\n",
    "\n",
    "    return traindata_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將資料分為訓練及測試資料\n",
    "def Dataset(data_list):\n",
    "    article_id_list = [ id for id in range(len(data_list))]\n",
    "    traindata_list, testdata_list, traindata_article_id_list, testdata_article_id_list = train_test_split(data_list, article_id_list, test_size=0.3, random_state=42)\n",
    "    train_texts = [[word[0] for word in doc] for doc in traindata_list]\n",
    "    train_tags = [[word[1] for word in doc] for doc in traindata_list]\n",
    "    test_texts = [[word[0] for word in doc] for doc in testdata_list]\n",
    "    test_tags = [[word[1] for word in doc] for doc in testdata_list]\n",
    "    return train_texts, train_tags, test_texts, test_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將文章依照 ['。', '？', '！', '，'] 斷句\n",
    "def split_list_by_value(text, tags):\n",
    "    split_list = ['。', '？', '！', '，']\n",
    "    sentence = []\n",
    "    sentence_tags = []\n",
    "    text_result = []\n",
    "    tag_result = []\n",
    "    \n",
    "    for i in range(len(text)):\n",
    "        sentence.append(text[i])\n",
    "        sentence_tags.append(tags[i])\n",
    "        if text[i] in split_list:\n",
    "            text_result.append(sentence.copy())\n",
    "            tag_result.append(sentence_tags.copy())\n",
    "            sentence.clear()\n",
    "            sentence_tags.clear()\n",
    "            \n",
    "    return text_result, tag_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將文章切為句子\n",
    "def transferDataFormat(texts, tags):\n",
    "    result_texts = []\n",
    "    result_tags = []\n",
    "    for id in range(len(texts)):\n",
    "        text, tag = split_list_by_value(texts[id], tags[id])\n",
    "        result_texts.extend(text)\n",
    "        result_tags.extend(tag)\n",
    "    return result_texts, result_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadTrainingDataPipeline(dataset):\n",
    "    train_texts, train_tags, test_texts, test_tags = Dataset(dataset)\n",
    "    train_texts, train_tags = transferDataFormat(train_texts, train_tags)\n",
    "    test_texts, test_tags = transferDataFormat(test_texts, test_tags)\n",
    "    return train_texts, train_tags, test_texts, test_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將tag轉為encoding\n",
    "def encode_tags(tags, encodings, max_length):\n",
    "    labels = [[TAGS2ID[tag] for tag in doc] for doc in tags]\n",
    "    encoded_labels = []\n",
    "    for label in labels:\n",
    "        sentence_label = np.array([-100])\n",
    "        sentence_label = np.append(sentence_label, label)\n",
    "        append_len = abs(max_length - len(sentence_label)) #max_length - len(sentence_label)\n",
    "        append_label = np.ones(append_len, dtype=int) * -100\n",
    "        encoded_labels.append(np.append(sentence_label, append_label).tolist())\n",
    "    return encoded_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將資料轉Trainer輸入格式\n",
    "def transferTrainingData(encodings, labels):\n",
    "    t_inputs = torch.tensor(encodings.input_ids)\n",
    "    t_masks = torch.tensor(encodings.attention_mask)\n",
    "    t_labels = torch.tensor(labels)\n",
    "\n",
    "    training_data = TensorDataset(t_inputs, t_masks, t_labels)\n",
    "    return training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadPredictFile(path):\n",
    "    trainingset = list()  # store trainingset [content,content,...]\n",
    "    position = list()  # store position [article_id, start_pos, end_pos, entity_text, entity_type, ...]\n",
    "    mentions = dict()  # store mentions[mention] = Type\n",
    "\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        file_text = f.read().encode('utf-8').decode('utf-8-sig')\n",
    "    \n",
    "    datas = file_text.split('\\n\\n--------------------\\n\\n')[:-1]\n",
    "    trainingset = []\n",
    "    for data in datas:\n",
    "        data = data.split('\\n')\n",
    "        trainingset.append(data[1:])\n",
    "    return trainingset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictData_split_to_sentence(texts):\n",
    "    split_list = ['。', '？', '！', '，']\n",
    "    sentence = []\n",
    "    result = []\n",
    "    for text in texts:\n",
    "        for i in range(len(text)):\n",
    "            sentence.append(text[i])\n",
    "            if text[i] in split_list:\n",
    "                result.append(sentence.copy())\n",
    "                sentence.clear()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentence(sentence):\n",
    "    inputs = tokenizer.encode_plus(sentence, is_split_into_words=True, return_tensors='pt')\n",
    "    inputs.to(DEVICE)\n",
    "    outputs = model(**inputs)\n",
    "    logits = np.array(outputs.logits.tolist())\n",
    "    result = np.argmax(logits, axis=2)\n",
    "    result = [ID2TAGS[i] for i in result[0]]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bert Training utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 取得BERT參數量\n",
    "def get_learnable_params(module):\n",
    "    model_params = [p for p in module.parameters() if p.requires_grad]\n",
    "    clf_params = [p for p in module.classifier.parameters() if p.requires_grad]\n",
    "\n",
    "    print(f\"\"\"\n",
    "    整個分類模型的參數量：{sum(p.numel() for p in model_params)}\n",
    "    線性分類器的參數量：{sum(p.numel() for p in clf_params)}\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 比較預測出的結果跟實際差別\n",
    "def outputResult(data):\n",
    "    predictions, labels, _ = trainer.predict(data)\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [ENTITY_TYPES[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [ENTITY_TYPES[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    print(classification_report(true_labels, true_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer使用compute_metrics\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [ID2TAGS[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [ID2TAGS[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = METRIC.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer使用data_collator\n",
    "def data_collator(dataset):\n",
    "    batch = {}\n",
    "    batch['input_ids'] = torch.stack([f[0] for f in dataset])\n",
    "    batch['attention_mask'] = torch.stack([f[1] for f in dataset])\n",
    "    batch['labels'] = torch.stack([f[2] for f in dataset])\n",
    "    \n",
    "    return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = loadData('../data/SampleData_deid.txt')\n",
    "training_data.extend(loadData('../data/train_1_update.txt'))\n",
    "training_data.extend(loadData('../data/train_2.txt'))\n",
    "training_data.extend(loadData('../data/sample_id_2.txt'))\n",
    "training_data.extend(loadData('../data/sample_contact.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, train_tags, test_texts, test_tags = loadTrainingDataPipeline(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bert Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = max(len(max(train_texts, key=len)), len(max(test_texts, key=len))) + 2    # 設定最大長度，比較訓練集及測試集最大長度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_LENGTH = 512 if MAX_LENGTH > 512 else MAX_LENGTH\n",
    "MAX_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ENTITY_TYPES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = BertConfig.from_pretrained(\n",
    "    MODEL['MODEL_VERSION'],\n",
    "    architectures=['BertForTokenClassification'],\n",
    "    hidden_dropout_prob=0.2,\n",
    "    attention_probs_dropout_prob=0.2,\n",
    "    num_labels=len(ENTITY_TYPES),\n",
    "    id2label=ID2TAGS,\n",
    "    label2id=TAGS2ID\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(MODEL['MODEL_VERSION'])\n",
    "model = BertForTokenClassification.from_pretrained(MODEL['MODEL_VERSION'], config=configuration).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return input_ids, token_type_ids, attention_mask\n",
    "train_encodings = tokenizer(train_texts, is_split_into_words=True, max_length=MAX_LENGTH, padding='max_length')\n",
    "val_encodings = tokenizer(test_texts, is_split_into_words=True, max_length=MAX_LENGTH, padding='max_length')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = encode_tags(train_tags, train_encodings, MAX_LENGTH)\n",
    "val_labels = encode_tags(test_tags, val_encodings, MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert all data into torch tensors, required data type for our model\n",
    "train_inputs = torch.tensor(train_encodings.input_ids)\n",
    "train_masks = torch.tensor(train_encodings.attention_mask)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "\n",
    "valid_inputs = torch.tensor(val_encodings.input_ids)\n",
    "valid_masks = torch.tensor(val_encodings.attention_mask)\n",
    "valid_labels = torch.tensor(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_dataloader = DataLoader(train_data, shuffle=False, batch_size=MODEL['BATCH_SIZE'])\n",
    "valid_data = TensorDataset(valid_inputs, valid_masks, valid_labels)\n",
    "valid_dataloader = DataLoader(valid_data, shuffle=False, batch_size=MODEL['BATCH_SIZE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    整個分類模型的參數量：101705509\n",
      "    線性分類器的參數量：28453\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "get_learnable_params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': MODEL['WEIGHT_DECAY']},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.0}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(\n",
    "    optimizer_grouped_parameters,\n",
    "    lr=MODEL['lr'],\n",
    "    eps=MODEL['eps']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total number of training steps is number of batches * number of epochs.\n",
    "total_steps = len(train_dataloader) * MODEL['EPOCHS']\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir = './output',\n",
    "    evaluation_strategy = \"epoch\", \n",
    "    learning_rate=MODEL['lr'],\n",
    "    per_device_train_batch_size=MODEL['BATCH_SIZE'],\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=MODEL['EPOCHS'],\n",
    "    weight_decay=MODEL['WEIGHT_DECAY']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizers = optimizer, scheduler\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=valid_data,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    optimizers=optimizers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='10734' max='10734' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10734/10734 55:44, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.036248</td>\n",
       "      <td>0.035959</td>\n",
       "      <td>0.621134</td>\n",
       "      <td>0.684335</td>\n",
       "      <td>0.651205</td>\n",
       "      <td>0.988336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.026392</td>\n",
       "      <td>0.029815</td>\n",
       "      <td>0.716482</td>\n",
       "      <td>0.705632</td>\n",
       "      <td>0.711016</td>\n",
       "      <td>0.989857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.029364</td>\n",
       "      <td>0.726961</td>\n",
       "      <td>0.802650</td>\n",
       "      <td>0.762933</td>\n",
       "      <td>0.991348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.012600</td>\n",
       "      <td>0.032574</td>\n",
       "      <td>0.780957</td>\n",
       "      <td>0.787979</td>\n",
       "      <td>0.784452</td>\n",
       "      <td>0.992247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.009416</td>\n",
       "      <td>0.035271</td>\n",
       "      <td>0.796357</td>\n",
       "      <td>0.806910</td>\n",
       "      <td>0.801598</td>\n",
       "      <td>0.992281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.007042</td>\n",
       "      <td>0.036906</td>\n",
       "      <td>0.798611</td>\n",
       "      <td>0.816375</td>\n",
       "      <td>0.807395</td>\n",
       "      <td>0.992631</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/biolab/anaconda3/envs/Aaron_env/lib/python3.6/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/biolab/anaconda3/envs/Aaron_env/lib/python3.6/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/biolab/anaconda3/envs/Aaron_env/lib/python3.6/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/biolab/anaconda3/envs/Aaron_env/lib/python3.6/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10734, training_loss=0.0215844102071965)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='2770' max='1385' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1385/1385 02:12]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.03690628707408905,\n",
       " 'eval_precision': 0.7986111111111112,\n",
       " 'eval_recall': 0.8163748225272125,\n",
       " 'eval_f1': 0.8073952726421718,\n",
       " 'eval_accuracy': 0.9926306278297029,\n",
       " 'epoch': 6.0}"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                precision    recall  f1-score   support\n",
      "\n",
      "            ID       0.84      0.72      0.78        29\n",
      "clinical_event       1.00      0.50      0.67         2\n",
      "       contact       0.85      0.76      0.80        29\n",
      "     education       1.00      1.00      1.00         2\n",
      "        family       0.67      0.62      0.64        29\n",
      "      location       0.94      0.93      0.94       148\n",
      "      med_exam       0.76      0.80      0.78       205\n",
      "         money       0.88      1.00      0.94        88\n",
      "          name       0.90      0.91      0.91       111\n",
      "  organization       0.00      0.00      0.00         1\n",
      "    profession       0.42      0.71      0.53         7\n",
      "          time       0.78      0.80      0.79      1462\n",
      "\n",
      "     micro avg       0.80      0.82      0.81      2113\n",
      "     macro avg       0.75      0.73      0.73      2113\n",
      "  weighted avg       0.80      0.82      0.81      2113\n",
      "\n"
     ]
    }
   ],
   "source": [
    "outputResult(valid_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_data = loadPredictFile('../data/test.txt')\n",
    "predict_data = list(map(lambda x: predictData_split_to_sentence(x), predict_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "docs = []\n",
    "for doc in predict_data:\n",
    "    article = []\n",
    "    for sentence in doc:\n",
    "        article.append(predict_sentence(sentence))\n",
    "    results.append(article.copy())\n",
    "    article.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('../data/result.txt', 'w') as f:\n",
    "    json.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/sentences.txt', 'w', encoding='utf8') as f:\n",
    "    json.dump(predict_data, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Aaron_env",
   "language": "python",
   "name": "aaron_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}